{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "import time\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/lee/Research/3.place_lbp_prediction_ISIE2019/Residual_Attention_Network/create_attention_model/data/cifar-10-python.tar.gz\n",
      "train_size:  50000 \n",
      "test_size:  10000\n",
      "('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
      "len class:  10\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop((32, 32), padding=4),   #left, top, right, bottom\n",
    "    # transforms.Scale(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='/home/lee/Research/3.place_lbp_prediction_ISIE2019/Residual_Attention_Network/create_attention_model/data/',\n",
    "                               train=True,\n",
    "                               transform=transform,\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='/home/lee/Research/3.place_lbp_prediction_ISIE2019/Residual_Attention_Network/create_attention_model/data/',\n",
    "                              train=False,\n",
    "                              transform=test_transform)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, # 64\n",
    "                                           shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'test' : test_loader    \n",
    "}\n",
    "dataset_sizes ={\n",
    "    'train': len(train_dataset),\n",
    "    'test' : len(test_dataset)\n",
    "}\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "print('train_size: ', dataset_sizes['train'], '\\ntest_size: ', dataset_sizes['test'])\n",
    "print(classes)\n",
    "print('len class: ', len(classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResidualUnit import ResidualBlock\n",
    "from ASPP_edit_PLACE_cifar10 import ASPP_places\n",
    "# from attention_module import Attention_step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_place(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_place, self).__init__()\n",
    "        self.begin_residual_blocks = nn.Sequential(\n",
    "            nn.Conv2d(3,64,kernel_size=3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding = 1)\n",
    "        )#16x16\n",
    "        self.trunk_first_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,64, kernel_size=1)\n",
    "        )#16x16\n",
    "        self.trunk = nn.Sequential(\n",
    "            ResidualBlock(64, 256, 1),\n",
    "            ResidualBlock(256, 256, 1)\n",
    "        )#16x16\n",
    "        self.trunk_last_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size = 1)\n",
    "        )#16x16\n",
    "        self.trunk_residual = nn.Sequential(\n",
    "            nn.Conv2d(64,256,1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.aspp = ASPP_places()\n",
    "        self.mask_first_conv = nn.Sequential(\n",
    "            nn.Conv2d(64, 96, kernel_size=1, stride = 2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )#28x28\n",
    "        self._last_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=1, stride = 2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 2048, kernel_size=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1)\n",
    "        )#4x4\n",
    "        \n",
    "        \n",
    "        self.mpool = nn.Sequential(\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=4, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(2048, len(classes))\n",
    "    def forward(self, x):\n",
    "        h = int(x.size()[2] / 2) #16\n",
    "        w = int(x.size()[3] / 2) #16\n",
    "#         print('h:',h, 'w:',w)\n",
    "        x1 = self.begin_residual_blocks(x)#16x16\n",
    "#         print(x1.shape)\n",
    "#         print(x1.shape)\n",
    "#################trunk_part#################\n",
    "        out_trunk1 = self.trunk_first_conv(x1)#16x16x64\n",
    "        out_trunk2 = self.trunk(out_trunk1)#16x16x256\n",
    "        out_trunk3 = self.trunk_last_conv(out_trunk2)#16x16x256\n",
    "        out_trunk4 = out_trunk3 + self.trunk_residual(x1)#16x16x256\n",
    "#################trunk_part#################        \n",
    "\n",
    "#################mask_part#################\n",
    "        feature_map = self.mask_first_conv(x1)#8x8x256\n",
    "        mask = self.aspp(feature_map)#8x8\n",
    "        mask = F.upsample(mask, size=(h, w), mode=\"bilinear\") #16x16x256\n",
    "#         mask = F.softmax(mask)\n",
    "#################mask_part#################\n",
    "        out1 = (1 + mask) * out_trunk4 #16x16x256\n",
    "#         print(out.shape)\n",
    "        out2 = self._last_conv(out1)#4x4x2048\n",
    "#         print(out.shape)\n",
    "        out3 = self.mpool(out2)#1x1x512\n",
    "#         print(out.shape)\n",
    "        out = out3.view(out3.size(0),-1)\n",
    "#         print(out.shape)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, out_trunk1, out_trunk2, out_trunk3, out_trunk4, feature_map, mask, out1, out2, out3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:         # Conv weight init\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:  # BatchNorm weight init\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention_place()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention_place(\n",
       "  (begin_residual_blocks): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (trunk_first_conv): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (trunk): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (conv4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (conv4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (trunk_last_conv): Sequential(\n",
       "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (trunk_residual): Sequential(\n",
       "    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "  )\n",
       "  (aspp): ASPP_places(\n",
       "    (conv_1x1_1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn_conv_1x1_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_3x3_dil6): Sequential(\n",
       "      (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_3x3_dil12): Sequential(\n",
       "      (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_3x3_dil18): Sequential(\n",
       "      (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_1x1_3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn_conv_1x1_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_1x1_4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (mask_first_conv): Sequential(\n",
       "    (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))\n",
       "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "  )\n",
       "  (_last_conv): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (mpool): Sequential(\n",
       "    (0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): AvgPool2d(kernel_size=4, stride=1, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA, model\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "       BatchNorm2d-5           [-1, 64, 16, 16]             128\n",
      "              ReLU-6           [-1, 64, 16, 16]               0\n",
      "            Conv2d-7           [-1, 64, 16, 16]           4,160\n",
      "       BatchNorm2d-8           [-1, 64, 16, 16]             128\n",
      "              ReLU-9           [-1, 64, 16, 16]               0\n",
      "           Conv2d-10           [-1, 64, 16, 16]           4,096\n",
      "      BatchNorm2d-11           [-1, 64, 16, 16]             128\n",
      "             ReLU-12           [-1, 64, 16, 16]               0\n",
      "           Conv2d-13           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-14           [-1, 64, 16, 16]             128\n",
      "             ReLU-15           [-1, 64, 16, 16]               0\n",
      "           Conv2d-16          [-1, 256, 16, 16]          16,384\n",
      "           Conv2d-17          [-1, 256, 16, 16]          16,384\n",
      "    ResidualBlock-18          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-19          [-1, 256, 16, 16]             512\n",
      "             ReLU-20          [-1, 256, 16, 16]               0\n",
      "           Conv2d-21           [-1, 64, 16, 16]          16,384\n",
      "      BatchNorm2d-22           [-1, 64, 16, 16]             128\n",
      "             ReLU-23           [-1, 64, 16, 16]               0\n",
      "           Conv2d-24           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-25           [-1, 64, 16, 16]             128\n",
      "             ReLU-26           [-1, 64, 16, 16]               0\n",
      "           Conv2d-27          [-1, 256, 16, 16]          16,384\n",
      "    ResidualBlock-28          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-29          [-1, 256, 16, 16]             512\n",
      "             ReLU-30          [-1, 256, 16, 16]               0\n",
      "           Conv2d-31          [-1, 256, 16, 16]          65,792\n",
      "           Conv2d-32          [-1, 256, 16, 16]          16,640\n",
      "      BatchNorm2d-33          [-1, 256, 16, 16]             512\n",
      "             ReLU-34          [-1, 256, 16, 16]               0\n",
      "           Conv2d-35             [-1, 96, 8, 8]           6,240\n",
      "      BatchNorm2d-36             [-1, 96, 8, 8]             192\n",
      "             ReLU-37             [-1, 96, 8, 8]               0\n",
      "           Conv2d-38            [-1, 128, 8, 8]          12,416\n",
      "      BatchNorm2d-39            [-1, 128, 8, 8]             256\n",
      "           Conv2d-40            [-1, 128, 8, 8]          12,416\n",
      "           Conv2d-41            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-42            [-1, 128, 8, 8]             256\n",
      "           Conv2d-43            [-1, 128, 8, 8]          12,416\n",
      "           Conv2d-44            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-45            [-1, 128, 8, 8]             256\n",
      "           Conv2d-46            [-1, 128, 8, 8]          12,416\n",
      "           Conv2d-47            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
      "           Conv2d-49            [-1, 256, 8, 8]         131,328\n",
      "      BatchNorm2d-50            [-1, 256, 8, 8]             512\n",
      "           Conv2d-51            [-1, 256, 8, 8]          65,792\n",
      "      ASPP_places-52            [-1, 256, 8, 8]               0\n",
      "           Conv2d-53            [-1, 512, 8, 8]         131,584\n",
      "      BatchNorm2d-54            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-55           [-1, 2048, 8, 8]       1,050,624\n",
      "      BatchNorm2d-56           [-1, 2048, 8, 8]           4,096\n",
      "             ReLU-57           [-1, 2048, 8, 8]               0\n",
      "        MaxPool2d-58           [-1, 2048, 4, 4]               0\n",
      "      BatchNorm2d-59           [-1, 2048, 4, 4]           4,096\n",
      "             ReLU-60           [-1, 2048, 4, 4]               0\n",
      "        AvgPool2d-61           [-1, 2048, 1, 1]               0\n",
      "           Linear-62                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 2,143,594\n",
      "Trainable params: 2,143,594\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 15.84\n",
      "Params size (MB): 8.18\n",
      "Estimated Total Size (MB): 24.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA, model\")\n",
    "    model.cuda() #after second other epoch model\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, scheduler, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        since1 = time.time()\n",
    "        # Each epoch has a training and validation phase\n",
    "        train_batches = len(dataloaders['train'])\n",
    "        for phase in ['train','test']:\n",
    "            \n",
    "            print(\"lr:\", optimizer.param_groups[0]['lr'])\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                if i % 100 == 0:\n",
    "                    print(\"\\rTraining batch {}/{}\".format(i, len(dataloaders[phase])), end='', flush=True)\n",
    "                # Use half training dataset\n",
    "                if i >= len(dataloaders[phase]):\n",
    "                    break    \n",
    "                inputs, labels = data\n",
    "                inputs, labels = Variable(inputs.cuda()),Variable(labels.cuda())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    out, out_trunk1, out_trunk2, out_trunk3, out_trunk4, feature_map, mask, out1, out2, out3 = model(inputs)\n",
    "                    _, preds = torch.max(out, 1)\n",
    "                    loss = criterion(out, labels)\n",
    "#                     print(loss)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('data/train_loss_places', epoch_loss, epoch)\n",
    "                writer.add_scalar('data/train_acc_places', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('data/test_loss_places', epoch_loss, epoch)\n",
    "                writer.add_scalar('data/test_acc_places', epoch_acc, epoch)\n",
    "            for name, param in model.named_parameters():\n",
    "                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "            time_elapsed1 = time.time() - since1\n",
    "            print('\\rEpoch process in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed1 // 60, time_elapsed1 % 60))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} lr: {:.8f}'.format(phase, epoch_loss, \n",
    "                                                                  epoch_acc*100, optimizer.param_groups[0]['lr']))\n",
    "#             csvfile = open(os.path.join('./csv/resnet18/', '20190102resnet18_64_data_15_places_{}{}_class{}_epoch{}.csv'.format(optimizer_name, learning_rate, len(class_names), num_epochs)), 'a', newline='')\n",
    "#             csv_writer = csv.writer(csvfile, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "#             csv_writer.writerow(['class', len(class_names), 'epoch', epoch, phase, epoch_loss, epoch_acc])\n",
    "#             csvfile.close()\n",
    "#             deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f} at {}'.format(best_acc, best_epoch+1))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "lr: 0.001\n",
      "Epoch process in 0m 28s\n",
      "train Loss: 1.0614 Acc: 62.4240 lr: 0.00100000\n",
      "lr: 0.001\n",
      "Epoch process in 0m 32s\n",
      "test Loss: 1.0363 Acc: 63.3000 lr: 0.00100000\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "lr: 0.001\n",
      "Epoch process in 0m 28s\n",
      "train Loss: 0.9264 Acc: 67.3060 lr: 0.00100000\n",
      "lr: 0.001\n",
      "Epoch process in 0m 32s\n",
      "test Loss: 1.0577 Acc: 62.2200 lr: 0.00100000\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "lr: 0.001\n",
      "Epoch process in 0m 28s\n",
      "train Loss: 0.8402 Acc: 70.4820 lr: 0.00100000\n",
      "lr: 0.001\n",
      "Epoch process in 0m 32s\n",
      "test Loss: 0.9450 Acc: 67.4500 lr: 0.00100000\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "lr: 0.001\n",
      "Epoch process in 0m 29s\n",
      "train Loss: 0.7594 Acc: 73.4680 lr: 0.00100000\n",
      "lr: 0.001\n",
      "Epoch process in 0m 32s\n",
      "test Loss: 0.9180 Acc: 67.6900 lr: 0.00100000\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "lr: 0.001\n",
      "Epoch process in 0m 29s\n",
      "train Loss: 0.6876 Acc: 76.1080 lr: 0.00100000\n",
      "lr: 0.001\n",
      "Epoch process in 0m 33s\n",
      "test Loss: 0.7069 Acc: 76.0200 lr: 0.00100000\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "lr: 0.001\n",
      "Epoch process in 0m 29s\n",
      "train Loss: 0.6478 Acc: 77.6120 lr: 0.00100000\n",
      "lr: 0.001\n",
      "Epoch process in 0m 32s\n",
      "test Loss: 0.7122 Acc: 75.8200 lr: 0.00100000\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "lr: 0.001\n",
      "Training batch 100/782"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "lr = 0.001  # 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "total_epoch = 100\n",
    "model_train = train_model(model, scheduler, criterion, optimizer, num_epochs=total_epoch)\n",
    "torch.save(model_train.state_dict(), './Trained/cifar10_adam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
